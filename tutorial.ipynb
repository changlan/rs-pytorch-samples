{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Reduction Server PyTorch Tutorial\n",
    "\n",
    "This notebook is a demonstration of how to use the Reduction Server feature on Vertex AI Training."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "First, install Vertex AI Python SDK"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "!pip install -U google-cloud-aiplatform"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: google-cloud-aiplatform in /usr/local/google/home/changlan/.local/lib/python3.9/site-packages (1.1.1)\n",
      "Collecting google-cloud-aiplatform\n",
      "  Using cached google_cloud_aiplatform-1.1.1-py2.py3-none-any.whl (1.2 MB)\n",
      "  Using cached google_cloud_aiplatform-1.1.0-py2.py3-none-any.whl (1.2 MB)\n",
      "Requirement already satisfied: packaging>=14.3 in /usr/local/google/home/changlan/.local/lib/python3.9/site-packages (from google-cloud-aiplatform) (21.0)\n",
      "Requirement already satisfied: google-cloud-storage<2.0.0dev,>=1.32.0 in /usr/local/google/home/changlan/.local/lib/python3.9/site-packages (from google-cloud-aiplatform) (1.41.1)\n",
      "Requirement already satisfied: proto-plus>=1.10.1 in /usr/local/google/home/changlan/.local/lib/python3.9/site-packages (from google-cloud-aiplatform) (1.19.0)\n",
      "Requirement already satisfied: google-cloud-bigquery<3.0.0dev,>=1.15.0 in /usr/local/google/home/changlan/.local/lib/python3.9/site-packages (from google-cloud-aiplatform) (2.23.1)\n",
      "Requirement already satisfied: google-api-core[grpc]<2.0.0dev,>=1.22.2 in /usr/local/google/home/changlan/.local/lib/python3.9/site-packages (from google-cloud-aiplatform) (1.31.1)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /usr/lib/python3/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform) (52.0.0)\n",
      "Requirement already satisfied: pytz in /usr/lib/python3/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform) (2021.1)\n",
      "Requirement already satisfied: google-auth<2.0dev,>=1.25.0 in /usr/local/google/home/changlan/.local/lib/python3.9/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform) (1.28.0)\n",
      "Requirement already satisfied: protobuf>=3.12.0 in /usr/lib/python3/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform) (3.12.4)\n",
      "Requirement already satisfied: six>=1.13.0 in /usr/local/google/home/changlan/.local/lib/python3.9/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform) (1.15.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/google/home/changlan/.local/lib/python3.9/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform) (1.53.0)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/lib/python3/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform) (2.25.1)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.29.0 in /usr/local/google/home/changlan/.local/lib/python3.9/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform) (1.39.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/google/home/changlan/.local/lib/python3.9/site-packages (from google-auth<2.0dev,>=1.25.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform) (4.2.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/lib/python3/dist-packages (from google-auth<2.0dev,>=1.25.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform) (0.2.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/google/home/changlan/.local/lib/python3.9/site-packages (from google-auth<2.0dev,>=1.25.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform) (4.7.2)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /usr/local/google/home/changlan/.local/lib/python3.9/site-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform) (1.3.1)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.4.1 in /usr/local/google/home/changlan/.local/lib/python3.9/site-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform) (1.7.2)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/google/home/changlan/.local/lib/python3.9/site-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform) (1.1.2)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /usr/local/google/home/changlan/.local/lib/python3.9/site-packages (from google-crc32c<2.0dev,>=1.0->google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform) (1.14.6)\n",
      "Requirement already satisfied: pycparser in /usr/local/google/home/changlan/.local/lib/python3.9/site-packages (from cffi>=1.0.0->google-crc32c<2.0dev,>=1.0->google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.20)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/lib/python3/dist-packages (from packaging>=14.3->google-cloud-aiplatform) (2.4.7)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /usr/lib/python3/dist-packages (from rsa<5,>=3.1.4->google-auth<2.0dev,>=1.25.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform) (0.4.8)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Before you run the following scripts, make sure you run `gcloud auth application-default login` in your terminal to authenticate the SDK."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "PROJECT = 'curious-entropy-222019'\n",
    "REGION = 'us-central1'\n",
    "API_ENDPOINT = f'{REGION}-aiplatform.googleapis.com'\n",
    "TRAINING_IMAGE = f'gcr.io/{PROJECT}/rs-test-pytorch:latest'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Build the training image. See `Dockerfile` for details about how to prepare your training image for reduction servers. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "!docker build -t $TRAINING_IMAGE .\n",
    "!docker push $TRAINING_IMAGE"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Step 1/8 : FROM gcr.io/deeplearning-platform-release/pytorch-gpu.1-7\n",
      " ---> 5bc07240b483\n",
      "Step 2/8 : WORKDIR /root\n",
      " ---> Using cache\n",
      " ---> cf847519748d\n",
      "Step 3/8 : RUN apt-get update &&     apt-get remove -y google-fast-socket &&     apt-get install -y libcupti-dev google-reduction-server\n",
      " ---> Using cache\n",
      " ---> d0bf607b199a\n",
      "Step 4/8 : ENV LD_LIBRARY_PATH=/usr/local/cuda/extras/CUPTI/lib64:${LD_LIBRARY_PATH}\n",
      " ---> Using cache\n",
      " ---> 012b86c7bb23\n",
      "Step 5/8 : ENV NCCL_DEBUG=INFO\n",
      " ---> Using cache\n",
      " ---> 70333525c831\n",
      "Step 6/8 : COPY mnist_trainer.py mnist_trainer.py\n",
      " ---> Using cache\n",
      " ---> bc8e4cfd76b1\n",
      "Step 7/8 : COPY run.sh run.sh\n",
      " ---> 793b80322286\n",
      "Step 8/8 : ENTRYPOINT [\"run.sh\"]\n",
      " ---> Running in d688a0d18086\n",
      "Removing intermediate container d688a0d18086\n",
      " ---> 7fe1943c85e2\n",
      "Successfully built 7fe1943c85e2\n",
      "Successfully tagged gcr.io/curious-entropy-222019/rs-test-pytorch:latest\n",
      "The push refers to repository [gcr.io/curious-entropy-222019/rs-test-pytorch]\n",
      "\n",
      "\u001b[1B\n",
      "\u001b[1B\n",
      "\u001b[1B\n",
      "\u001b[1B\n",
      "\u001b[1B\n",
      "\u001b[1B\n",
      "\u001b[1B\n",
      "\u001b[1B\n",
      "\u001b[1B\n",
      "\u001b[1B\n",
      "\u001b[1B\n",
      "\u001b[1B\n",
      "\u001b[1B\n",
      "\u001b[1B\n",
      "\u001b[1B\n",
      "\u001b[1B\n",
      "\u001b[1B\n",
      "\u001b[12B\n",
      "\u001b[1B\n",
      "\u001b[1B\n",
      "\u001b[14B\n",
      "\u001b[1B\n",
      "\u001b[15B\n",
      "\u001b[15B\n",
      "\u001b[1B\n",
      "\u001b[1B\n",
      "\u001b[15B\n",
      "\u001b[1B\n",
      "\u001b[14B\n",
      "\u001b[30Blatest: digest: sha256:f60675a7766a609626973d5eaad7a201fe6b7402a8750fdbaafd466bd0087a8e size: 6614\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create training jobs. In this example, we use two `a2-highgpu-8g` as worker nodes, and 8 `n1-highcpu-16` as reducer nodes."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "from google.cloud import aiplatform, aiplatform_v1beta1\n",
    "\n",
    "aiplatform.init(\n",
    "    # your Google Cloud Project ID or number environment default used is not set\n",
    "    project=PROJECT,\n",
    "\n",
    "    # the Vertex AI region you will use defaults to us-central1\n",
    "    location=REGION,\n",
    ")\n",
    "\n",
    "custom_job_spec = {\n",
    "   'display_name': 'reduction-server-job',\n",
    "   'job_spec': {\n",
    "       'worker_pool_specs': [\n",
    "           {\n",
    "               'container_spec': {\n",
    "                   'image_uri': TRAINING_IMAGE\n",
    "                },\n",
    "                'machine_spec': {\n",
    "                    'accelerator_count': 8,\n",
    "                    'accelerator_type': 'NVIDIA_TESLA_A100',\n",
    "                    'machine_type': 'a2-highgpu-8g'\n",
    "                },\n",
    "                'replica_count': 1\n",
    "            },\n",
    "            {\n",
    "                'container_spec': {\n",
    "                    'image_uri': TRAINING_IMAGE\n",
    "                },\n",
    "                'machine_spec': {\n",
    "                    'accelerator_count': 8,\n",
    "                    'accelerator_type': 'NVIDIA_TESLA_A100',\n",
    "                    'machine_type': 'a2-highgpu-8g'\n",
    "                },\n",
    "                'replica_count': 1\n",
    "            },\n",
    "            {\n",
    "                'container_spec': {\n",
    "                    'image_uri': 'us-docker.pkg.dev/vertex-ai-restricted/training/reductionserver:latest'\n",
    "                },\n",
    "                'machine_spec': {\n",
    "                    'machine_type': 'n1-highcpu-16'\n",
    "                },\n",
    "                'replica_count': 8\n",
    "            },\n",
    "        ]\n",
    "   }\n",
    "}\n",
    "\n",
    "options = dict(api_endpoint=API_ENDPOINT)\n",
    "client = aiplatform_v1beta1.services.job_service.JobServiceClient(client_options=options)\n",
    "parent = f\"projects/{PROJECT}/locations/{REGION}\"\n",
    "client.create_custom_job(\n",
    "   parent=parent, custom_job=custom_job_spec\n",
    ")\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "name: \"projects/697926852371/locations/us-central1/customJobs/6490966894575616000\"\n",
       "display_name: \"reduction-server-job\"\n",
       "job_spec {\n",
       "  worker_pool_specs {\n",
       "    machine_spec {\n",
       "      machine_type: \"a2-highgpu-8g\"\n",
       "      accelerator_type: NVIDIA_TESLA_A100\n",
       "      accelerator_count: 8\n",
       "    }\n",
       "    replica_count: 1\n",
       "    disk_spec {\n",
       "      boot_disk_type: \"pd-ssd\"\n",
       "      boot_disk_size_gb: 100\n",
       "    }\n",
       "    container_spec {\n",
       "      image_uri: \"gcr.io/curious-entropy-222019/rs-test-pytorch:latest\"\n",
       "    }\n",
       "  }\n",
       "  worker_pool_specs {\n",
       "    machine_spec {\n",
       "      machine_type: \"a2-highgpu-8g\"\n",
       "      accelerator_type: NVIDIA_TESLA_A100\n",
       "      accelerator_count: 8\n",
       "    }\n",
       "    replica_count: 1\n",
       "    disk_spec {\n",
       "      boot_disk_type: \"pd-ssd\"\n",
       "      boot_disk_size_gb: 100\n",
       "    }\n",
       "    container_spec {\n",
       "      image_uri: \"gcr.io/curious-entropy-222019/rs-test-pytorch:latest\"\n",
       "    }\n",
       "  }\n",
       "  worker_pool_specs {\n",
       "    machine_spec {\n",
       "      machine_type: \"n1-highcpu-16\"\n",
       "    }\n",
       "    replica_count: 8\n",
       "    disk_spec {\n",
       "      boot_disk_type: \"pd-ssd\"\n",
       "      boot_disk_size_gb: 100\n",
       "    }\n",
       "    container_spec {\n",
       "      image_uri: \"us-docker.pkg.dev/vertex-ai-restricted/training/reductionserver:latest\"\n",
       "    }\n",
       "  }\n",
       "}\n",
       "state: JOB_STATE_PENDING\n",
       "create_time {\n",
       "  seconds: 1627515593\n",
       "  nanos: 405568000\n",
       "}\n",
       "update_time {\n",
       "  seconds: 1627515593\n",
       "  nanos: 405568000\n",
       "}"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now you should be able to see your training job on Cloud Console."
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.2",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.2 64-bit"
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}